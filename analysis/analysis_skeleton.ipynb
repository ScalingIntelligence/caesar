{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Muli-Turn Analysis\n",
    "\n",
    "Example to build your analysis notebook to analyze multi-turn runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import caesar.analysis.analysis_utils as analysis_utils\n",
    "\n",
    "from caesar.utils import check_result_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_REPO_DIR = os.path.dirname(os.path.dirname(os.getcwd())) # TOP LEVEL REPO\n",
    "from monkeys.problems.kernelbench_gen_level_1 import (\n",
    "    DATASET as KERNELBENCH_LEVEL_1_DATASET,\n",
    "    SUBSET_DATASET as KERNELBENCH_LEVEL_1_SUBSET_DATASET,\n",
    ")\n",
    "from monkeys.problems.kernelbench_gen_level_2 import (\n",
    "    DATASET as KERNELBENCH_LEVEL_2_DATASET,\n",
    "    SUBSET_DATASET as KERNELBENCH_LEVEL_2_SUBSET_DATASET\n",
    ") \n",
    "from monkeys.problems.kernelbench_gen_level_3 import (\n",
    "    DATASET as KERNELBENCH_LEVEL_3_DATASET,\n",
    "    SUBSET_DATASET as KERNELBENCH_LEVEL_3_SUBSET_DATASET    \n",
    ") \n",
    "\n",
    "from monkeys.problems.problem_utils import KernelBenchDataset\n",
    "\n",
    "dataset_name_to_dataset = {\n",
    "    \"KernelBench/level1\": KERNELBENCH_LEVEL_1_DATASET,\n",
    "    \"KernelBench/level2\": KERNELBENCH_LEVEL_2_DATASET,\n",
    "    \"KernelBench/level3\": KERNELBENCH_LEVEL_3_DATASET,\n",
    "}\n",
    "\n",
    "dataset_name_to_subset_dataset = {\n",
    "    \"KernelBench/level1\": KERNELBENCH_LEVEL_1_SUBSET_DATASET,\n",
    "    \"KernelBench/level2\": KERNELBENCH_LEVEL_2_SUBSET_DATASET,\n",
    "    \"KernelBench/level3\": KERNELBENCH_LEVEL_3_SUBSET_DATASET,\n",
    "}\n",
    "\n",
    "\n",
    "MULTI_TURN_BASE_LOG_DIR = \"/matx/u/simonguo/kernel_multi_turn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ex_run_group = \"trial_level1_reflection_all_prev_deepseek\"\n",
    "dataset_name = \"KernelBench/level1\"\n",
    "level = 1\n",
    "use_subset = False\n",
    "\n",
    "# with greedy \n",
    "# we will do num_sample = 1\n",
    "num_sample = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataset object\n",
    "dataset = KernelBenchDataset(\n",
    "    dataset_name=dataset_name,    \n",
    "    level=level, \n",
    "    use_subset=use_subset,  # Use the checkbox value instead of config value\n",
    "    dataset=dataset_name_to_dataset[dataset_name], \n",
    "    subset_dataset=dataset_name_to_subset_dataset[dataset_name]\n",
    "    )\n",
    "\n",
    "# dataset.get_problem_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_run_name = \"trial_run_turns_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the runs in the run group\n",
    "runs = analysis_utils.get_available_runs(os.path.join(MULTI_TURN_BASE_LOG_DIR, ex_run_group))\n",
    "\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_data(run_group, run_name, dataset: KernelBenchDataset):\n",
    "    # get the log data\n",
    "\n",
    "    run_data = []\n",
    "\n",
    "    for problem_id in dataset.get_problem_ids(): # logical ID\n",
    "        for sample_id in range(num_sample):\n",
    "\n",
    "            log_path = os.path.join(MULTI_TURN_BASE_LOG_DIR, run_group, run_name, f\"problem_{problem_id}\", f\"sample_{sample_id}\", \"log.json\")\n",
    "            if not check_result_exists(log_dir_prefix=MULTI_TURN_BASE_LOG_DIR, run_group=run_group, run_name=run_name, problem_id=problem_id, sample_id=sample_id):\n",
    "                print(f\"Result not found for {run_name} {problem_id} {sample_id}\")\n",
    "                continue\n",
    "\n",
    "            config_path = os.path.join(MULTI_TURN_BASE_LOG_DIR, run_group, run_name, f\"problem_{problem_id}\", f\"sample_{sample_id}\", \"config.json\")\n",
    "            config_data = analysis_utils.load_run_data(config_path)\n",
    "            \n",
    "            # you can do something with those \n",
    "\n",
    "\n",
    "            # this is the log data for the particular problem and sample\n",
    "            log_data = analysis_utils.load_run_data(log_path)\n",
    "            num_rounds = log_data[\"metadata\"][\"num_rounds\"]\n",
    "            # get the trajectory of compilation, correctness, and runtime over turns\n",
    "            turn_compile_trajectory, turn_correct_trajectory, turn_runtime_trajectory = analysis_utils.get_turn_trajectory_overviews(log_data)\n",
    "\n",
    "            # maybe you can pick the best one here\n",
    "            # print(f\"Turn Compile Trajectory: {turn_compile_trajectory}\" )\n",
    "            # print(f\"Turn Correct Trajectory: {turn_correct_trajectory}\")\n",
    "            # print(f\"Turn Runtime Trajectory: {turn_runtime_trajectory}\")\n",
    "\n",
    "            # you can get the final result here\n",
    "            # as this is the last one\n",
    "            final_result = log_data[\"result\"][\"eval_result\"]\n",
    "            \n",
    "            # print(f\"Final Result: {final_result}\")\n",
    "            # you can do something with this \n",
    "            \n",
    "            # you can return a new dict and then make a dataframe in the end\n",
    "            run_data.append({\n",
    "                \"problem_id\": problem_id,\n",
    "                \"sample_id\": sample_id,\n",
    "                \"num_rounds\": num_rounds,\n",
    "                \"final_result\": final_result,\n",
    "                \"turn_compile_trajectory\": turn_compile_trajectory,\n",
    "                \"turn_correct_trajectory\": turn_correct_trajectory,\n",
    "                \"turn_runtime_trajectory\": turn_runtime_trajectory,\n",
    "            })\n",
    "        \n",
    "    return run_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data = get_run_data(run_group=ex_run_group, run_name=ex_run_name, dataset=dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the run data list to a pandas DataFrame\n",
    "run_data_df = pd.DataFrame(run_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "run_data_df\n",
    "\n",
    "# this is parituclar to this config, num_rounds, and the final resultZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i have a helper function for you load baseline \n",
    "# you can compute score here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-monkeys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
